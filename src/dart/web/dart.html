<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Fake Startup Guru</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">

        <link rel="stylesheet" href="css/min/bootstrap.css">
        <link rel="stylesheet" href="css/min/responsive.css">
        
        <link href='http://fonts.googleapis.com/css?family=Lato:400,700,900|Trocchi|Noticia+Text:400,400italic,700,700italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
        
        <link rel="stylesheet" href="dart.css">

        <!-- import the text-generator -->
        <link rel="components" href="xtextgenerator.html">
        
        <link href="data:image/x-icon;base64,AAABAAEAEBAQAAAAAAAoAQAAFgAAACgAAAAQAAAAIAAAAAEABAAAAAAAgAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAsK6sAK1YAwBUU1IA////AOORQABKPjIA/9ChAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABmZmZmZgAAAGERERM2AAAAYREREzYAAABmZmZmZgAAAABhETYAAAAAAAYTYAAAAAZmZmZmZmAABnVVVVVSYAAGdVVVVVJgAAZ1VVVVUmAABnVVVVVSYAAGRVVVVVJgAAZEd3d3cmAABmZmZmZmYAAAAAAAAAAAD//wAA4AcAAOAHAADgBwAA4AcAAPgfAAD8PwAAwAMAAMADAADAAwAAwAMAAMADAADAAwAAwAMAAMADAAD//wAA" rel="icon" type="image/x-icon" />
    </head>
    <body data-spy="scroll" data-target=".navbar">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="http://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->
        <div class="navbar navbar-transparent navbar-fixed-top">
            <a href="https://github.com/asimihsan/fakestartupguru"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_green_007200.png" alt="Fork me on GitHub"></a>
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <div class="nav-collapse collapse pull-right">
                        <ul class="nav">
                            <li class="active"><a href="#generator">Generator</a></li>
                            <li><a href="#about">About</a></li>
                            <li><a href="#details">Details</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </div>
        </div>
        <div class="body">
            <section id="generator">
                <div class="container">
                    <div class="row">
                        <div class="span8 offset2">
                            <div class="hero">
                                <!-- Main hero unit for a primary marketing message or call to action -->
                                <h1>Fake Startup Guru</h1>
                                <h2>Generate text based on the biographies of SW judges and mentors</h2>
                                <div is="x-text-generator" id="text_generator" class="clearfix"></div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            <section class="blue" id="about">
                <div class="container">
                    <div class="row">
                        <div class="span10 offset1 center">
                            <h2 class="cursive shadow-text">About</h2>
                        </div>
                        <div class="span10 offset1">
<p>Using the <a href="https://github.com/StartupWeekend/googleio_contest">Startup Weekend Operations and Organizers Portal</a> (SWOOP) dataset I scraped every Startup Weekend (SW) event's webpage for the past year and used <a href="http://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> methods to create <a href="http://en.wikipedia.org/wiki/Generative_model">generative models</a> based on the language used. I've then used these models to create not-so-random gibberish.</p>
<p>In order to create this page I've used <a href="http://www.dartlang.org/">Google Dart</a>, which is easy enough to both learn and use within 24 hours and also compiles down to <a href="http://en.wikipedia.org/wiki/JavaScript">JavaScript</a>. Moreover in order to compress the HTML/CSS/JS resources as much as possible I've used the <a href="http://googledevelopers.blogspot.co.uk/2013/02/compress-data-more-densely-with-zopfli.html">Zopfli algorithm</a> developed by Google Software Engineer Lode Vandevenne, via the <a href="http://zlib.net/pigz/">pigz</a> command-line tool.</p>
<p>I'd like to thank Professor Michael Collins and the classmates of the <a href="https://www.coursera.org/course/nlangp">Natural Language Processing course on Coursera</a> for opening up the world of NLP to me!</p>
                        </div>
                    </div>
                </div>
            </section>
            <section class="white" id="details">
                <div class="container">
                    <div class="row">
                        <div class="span10 offset1 center">                            
                            <h2 class="cursive shawdow-text">Details</h2>
                         </div>
                         <div class="span10 offset1">

<h3 id="introduction">Introduction</h3>
<p>Which of the following two English sentences is more likely?</p>
<pre><code>the dog saw the man with the telescope</code></pre>
<p>or:</p>
<pre><code>telescope man dog the with a saw</code></pre>
<p>It seems obvious that the first sentence is more likely, as the second sentence is grossly grammatically incorrect. However, how about this third sentence:</p>
<pre><code>the man saw the dog with the telescope</code></pre>
<p>This raises the stakes considerably. Not only is the third sentence perfectly valid and perhaps <em>more</em> plausible than the first, but in fact it makes one aware of just how ambiguous the sentences actually are. In the first sentence, is the dog seeing a man with a telescope, or is the dog using a telescope to see a man?</p>
<p>In a very real sense the <strong>ambiguity</strong> of natural languages, like English, are unresolvable if we &quot;just&quot; stare at the text. We need some background, some <strong>context</strong> if you will, that tells us that we live in a world where dogs are not likely to be using telescopes to see men.</p>
<p>I will now attempt to summarise the methods I've used to bring understanding to natural languages. My summary below will be quite cursory so for more details please see <a href="http://files.asimihsan.com/courses/nlp-coursera-2013/notes/nlp.html">my online course notes</a> on the Natural Language Processing course on Coursera, in particular the material for weeks 1 and 2.</p>
<h3 id="language-models">Language models</h3>
<p>The most direct way to get our context is to examine a set of real sentences in a language we're interested in and then make a proverbial map of them. Just like maps do not record every little minute detail about the terrain, we want our models to somehow capture the &quot;essence&quot; or &quot;je ne sais quoi&quot; of the language.</p>
<p>However, our models will be very different to maps in one crucial respect. A map can tell you &quot;if you see a tree there, and a hill here, then you're probably here&quot;, i.e. it is a <strong>discriminative</strong> model. We couldn't possibly use a map to create an entirely new world because the map <em>hasn't captured anything</em> that would let us do that. We want our language models to be <a href="http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm"><strong>generative</strong></a>, and let us create new sentences that &quot;sort of&quot; look like the underlying language.</p>
<h3 id="n-gram-language-models">N-Gram Language Models</h3>
<p>In an ideal world we would have hundreds of billions of sentences, each completely different from one another. In the real world often we do not have much data; in this case I had access to around 5,200 biographies for either judges or mentors who participated in a Startup Weekend somewhere in the world over the past year. Of those around 2,000 were either not in English or not interesting, for example too short or just bullet points of words. This leaves us with a measley 3,200 biographies!</p>
<p>How can we make a model of something as complicated as English if we don't have enough data? One way is to assume that a sentence is a sequence of words, and the likelihood of a sentence depends on the likelihood of smaller <em>chunks</em> within the sentence. For example, in a bigram (or 2-gram) model, with the sentence:</p>
<pre><code>The dog ate grapes.</code></pre>
<p>We could say that the probability of the sentence in the English language is:</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; p(\textrm{The, dog, ate, grapes}) \\
      = &amp; p(\textrm{START, The}) \times p(\textrm{The, dog}) \\ 
        \times &amp; p(\textrm{dog, ate}) \times p(\textrm{ate, grapes}) \\
        \times &amp; p(\textrm{grapes, STOP})
    \end{aligned}
\end{align}
\]</span></p>
<p>where START and STOP are special convenience symbols that indicate the start and end of a sentence. Notice how we're now more likely to have much more data, for example the number of sentences that start with &quot;the&quot;, however we've made a massive assumption about how sentences &quot;work&quot; in English.</p>
<p>Let's suppose this model is &quot;correct&quot;. What are the individual probabilities. For example, what's the probability that a sentence begins with &quot;the&quot;? The models used above are &quot;maximum likelihood&quot; estimates, meaning that:</p>
<p><span class="math">\[
p(\textrm{START, the}) = \frac{\textrm{Count(START, the)}}{\textrm{Count(START)}}
\]</span></p>
<p>This is very easy to pull off, and is how the &quot;Bigram Maximum Likelihood&quot; and &quot;Trigram Maximum Likelihood&quot; generator models above work.</p>
<h3 id="sparsity">Sparsity</h3>
<p>There is an added catch with the trigram case. Consider a data set with the following sentences:</p>
<pre><code>the dog chased the cat
the cat chased the dog
the man read a paper</code></pre>
<p>Under a trigram maximum-likelihood language model, what is the probability of the following sentence?</p>
<pre><code>the man chased the cat</code></pre>
<p>One part of the probabilities must be the phrase &quot;man chased the&quot;, i.e.:</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        p(\textrm{man chased the}) = &amp; \frac{\textrm{Count(man chased the)}}{\textrm{Count(man chased)}} \\
      = &amp; \frac{0}{0} 
    \end{aligned}
\end{align}
\]</span></p>
<p>Hence there is the possibility of undefined results for some probabilities. This is tedious! Perhaps with much more data we'd just happen to come across this &quot;the man&quot; somewhere, but this isn't a sure-fire solution.</p>
<p>In fact the resolution brings a lot of understanding to the problem. Simply because &quot;man chased that&quot; or &quot;man chased&quot; have never occurred before doesn't mean that <em>they're impossible</em>. How do we let our model figuratively shrug and say &quot;well you know I <em>should</em> be putting zero here...but I'll give you a pass&quot;?</p>
<p>One of the simplest ways is to say that if a given word is &quot;rare&quot;, e.g. has been seen less than 5 times, we &quot;change&quot; it to &quot;<strong>RARE</strong>&quot; or some other special symbol. More advanced methods assign such words to specific rare word classes like &quot;<strong>FOUR_DIGITS</strong>&quot; or &quot;<strong>ALL_CAPITALS</strong>&quot;. Please see <a href="http://files.asimihsan.com/courses/nlp-coursera-2013/notes/nlp.html#dealing-with-low-frequency-words-an-example">Dealing with Low-Frequency Words: An Example</a> in my notes for more details.</p>
<p>In this case I've assigned words that occur less than or equal to 2 times to a <strong>RARE</strong> token when both training the models and generating new sentences.</p>
<h3 id="hidden-markov-models">Hidden Markov Models</h3>
<p>TODO; please read my class notes for more information. I implemented a Hidden Markov Model such that <strong>part-of-speech</strong> (POS) tags are &quot;transmitted&quot; according to a trigram mode, and each POS tag uses a bigram count to &quot;emit&quot; a word. You'll note that the output of the &quot;Trigram Hidden Markov Model&quot; seems much more creative but a little worse than the &quot;Trigram Maximum Likelihood&quot; model.</p>
<h3 id="future-work">Future work</h3>
<ul>
<li>Implement linear interpolation and Katz backoff language models with cross-validation. I'd expect these to be quite superior, so this is the obvious next step.</li>
<li>Currently the Hidden Markov Model (HMM) uses trigram transmissions for the POS tags and bigram emissions for a word per tag. I think this model would be considerably improved if one used trigram emissions, such that a word is emitted depending on the previous two tags.</li>
<li>I've calculated the perplexities of these models; rougly it's 520 for the unigram model, 6 for the bigram model, 5 for the trigram. The bigram and trigram numbers are far too low so I've made a mistake, and it's worth going back and fixing this.</li>
<li>Visualisations for the data?</li>
</ul>

                        </div>
                    </div>
                </div>
            </section>
        </div>

        <footer>
            <ul class="unstyled links center">
                <li>
                    <a href="http://www.asimihsan.com">
                        Made by Asim Ihsan
                    </a>
                </li>
                <li>
                    <a href="https://www.coursera.org/course/nlangp">
                        Natural Language Processing on Coursera
                    </a>
                </li>
            </ul>
        </footer>

        <script type="application/dart" src="dart.dart"></script>
        <script src="packages/browser/dart.js"></script>

        <script src="http://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js" type="text/javascript"></script>
        <script>window.jQuery || document.write('<script src="js/jquery-1.8.2.min.js"><\/script>')</script>
        <script type="text/javascript" src="js/bootstrap-scrollspy.js"></script>

        <script type="text/javascript" src="js/bootstrap-dropdown.js"></script>

        <script type="text/javascript" src="js/bootstrap-collapse.js"></script>

        <script type="text/javascript" src="js/fitvid.js"></script>
        <script type="text/javascript" src="js/launch.js"></script>
        
        <script type="text/javascript"
          src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
        
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        
          ga('create', 'UA-39936765-1', 'fakestartupguru.com');
          ga('send', 'pageview');
        
        </script>

    </body>
</html>
